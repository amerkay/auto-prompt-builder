{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROMPT Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My current code\n",
    "```python\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def extract_sections(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Pattern to identify headings and sections\n",
    "    pattern = r'(#+ .+?)(?=\\n#+ |\\Z)'\n",
    "\n",
    "    # Extract sections using regular expressions\n",
    "    sections = re.findall(pattern, content, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "    # Prepare data for CSV\n",
    "    data = [{'Output': section.strip()} for section in sections]\n",
    "\n",
    "    # Write data to a CSV file\n",
    "    with open('extracted_sections.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=['Output'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Replace with your actual file path\n",
    "file_path = './(tmp) Salary Article MD.md'\n",
    "extract_sections(file_path)\n",
    "```\n",
    "\n",
    "## From the LangChain docs\n",
    "\n",
    "The simplest composition is just combining a prompt and model to create a chain that takes user input, adds it to a prompt, passes it to a model, and returns the raw model output.\n",
    "\n",
    "```\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\n",
    "model = ChatOpenAI()\n",
    "chain = prompt | model\n",
    "```\n",
    "\n",
    "```\n",
    "chain.invoke({\"foo\": \"bears\"})\n",
    "```\n",
    "\n",
    "## Rewriting Prompt:\n",
    "\n",
    "```md\n",
    "Read my article section:\n",
    "___START_OF_SECTION___\n",
    "{article_section}\n",
    "___END_OF_SECTION___\n",
    "\n",
    "## YOUR TASK:\n",
    "\n",
    "Rewrite it in a **completely different writing style**. \n",
    "\n",
    "All pieces of information and links must be preserved. \n",
    "The heading hierarchy must be preserved, for example heading 2's must stay heading 2's and so on. \n",
    "Tables and bullet points must be written in a different structure.\n",
    "```\n",
    "\n",
    "## TASK\n",
    "\n",
    "Write python code that reads the output file `extracted_sections.csv`, and for each row/value of `Output`, adds two columns:\n",
    "- `ID`, starting at 1.\n",
    "- `Input`, which includes the reply of gpt-4 using LangChain to the `Rewriting Prompt` above. `{article_section}` is the section markdown from the `Output` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "# Initialize LangChain\n",
    "# output_parser = StrOutputParser()\n",
    "# model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.7, max_tokens=2000)\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3, max_tokens=2000)\n",
    "\n",
    "# set_llm_cache(SQLiteCache(database_path=\"../.langchain.db\"))\n",
    "\n",
    "# Load the WRITEP prompt and set up the LangChain chain\n",
    "# rewrite_prompt = load_prompt(\"PROMPT_REWRITE_STYLE.json\")\n",
    "# chain = rewrite_prompt | model | output_parser\n",
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def extract_sections(path_input, path_output):\n",
    "    with open(path_input, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    pattern = r'(#+ .+?)(?=\\n#+ |\\Z)'\n",
    "    sections = re.findall(pattern, content, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "    data = {'ID': [], 'INPUT: TEXT_1': []}\n",
    "    accumulated_section = \"\"\n",
    "    section_id = 1\n",
    "\n",
    "    for section in sections:\n",
    "        if word_count(accumulated_section + section) < 300:\n",
    "            accumulated_section += \"\\n\\n\" + section\n",
    "            accumulated_section = accumulated_section.strip()\n",
    "        else:\n",
    "            # print(f'Processing section {section_id}...')\n",
    "            # rewritten_section = chain.invoke({\"article_section\": accumulated_section})\n",
    "            data['ID'].append(section_id)\n",
    "            data['INPUT: TEXT_1'].append(accumulated_section)\n",
    "            # data['Input'].append(rewritten_section)\n",
    "            accumulated_section = section.strip()\n",
    "            section_id += 1\n",
    "    \n",
    "    # Process any remaining accumulated section\n",
    "    if accumulated_section:\n",
    "        # accumulated_section = accumulated_section\n",
    "        # print(f'Processing section {section_id}...')\n",
    "        # rewritten_section = chain.invoke({\"article_section\": accumulated_section})\n",
    "        data['ID'].append(section_id)\n",
    "        data['INPUT: TEXT_1'].append(accumulated_section)\n",
    "        # data['Input'].append(rewritten_section)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # df.to_csv('dataset-writing-style-salary-veronica.csv', index=False, encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
    "    df.to_excel(path_output, index=False)\n",
    "\n",
    "# Replace with your actual file path\n",
    "path_input = './test_not_v_3.md'\n",
    "path_output = 'writing-style-not_v_3.xlsx'\n",
    "extract_sections(path_input, path_output)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.0, max_tokens=2000)\n",
    "# model_eval = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0, max_tokens=500)\n",
    "\n",
    "# Load the PROMPT_EVAL_WRITING_STYLE and set up the LangChain chain\n",
    "eval_prompt = load_prompt(\"PROMPT_EVAL_WRITING_STYLE.json\")\n",
    "chain = eval_prompt | model_eval | output_parser\n",
    "\n",
    "# Load the markdown file as a string\n",
    "with open('test_not_v.md', 'r') as file:\n",
    "    md_string = file.read().replace('\\n', ' ')\n",
    "\n",
    "def evaluate_section(row):\n",
    "    print(f'Evaluating section {row[\"ID\"]}...')\n",
    "    evaluation = chain.invoke(\n",
    "        {\"TEXT_1\": row[\"Output\"], \"TEXT_2\": row[\"Input\"]}\n",
    "        # {\"TEXT_1\": row[\"Input\"], \"TEXT_2\": row[\"Output\"]}\n",
    "        # {\"TEXT_1\": row[\"Output\"], \"TEXT_2\": md_string}\n",
    "        # {\"TEXT_1\": md_string, \"TEXT_2\": row[\"Output\"]}\n",
    "    )\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "# For each section, evaluate the writing style\n",
    "df = pd.read_excel(path_output)\n",
    "# df = df.drop(columns=['Input'])\n",
    "df[\"Evaluation\"] = df.apply(evaluate_section, axis=1)\n",
    "\n",
    "# Extract the score using regex. Example `SCORE: 9` - score is an integer between 0 and 10\n",
    "pattern = r\"SCORE: (\\d+)\"\n",
    "df[\"Score\"] = df[\"Evaluation\"].str.extract(pattern, expand=False).astype(int)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
