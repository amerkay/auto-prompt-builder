{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPrompt - Auto Write Evaluation Prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to know what the right prompt is, and it's hard to know if you've found it. This project aims to automate the process of finding the perfect evaluation prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "from utils import save_log_file, load_model\n",
    "from data_handling import load_and_clean_dataset\n",
    "\n",
    "from strategies.sequential import SequentialStrategy\n",
    "from strategies.basic import BasicStrategy\n",
    "from strategies.tree_search import TreeSearchStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_FILE = \"./datasets/sentiment_analysis_examples_25.csv\"\n",
    "# DATASET_FILE = \"./datasets/dataset-writing-style-v-not-v.xlsx\"\n",
    "DATASET_FILE = \"./datasets/writing-style.xlsx\"\n",
    "# DATASET_FILE = \"./datasets/writing-style-30-100-words.xlsx\"\n",
    "\n",
    "# Seed Idea for prompt generation\n",
    "# IDEA_SEED = \"\"\"Decide the sentiment of the input text.\"\"\"\n",
    "IDEA_SEED = \"\"\"Compare the two pieces of text. Your OUTPUT MUST ONLY take the writing style, voice, tone and stucture into consideration. Do not consider the meaning or thematic similarity of the texts.\"\"\".strip()\n",
    "\n",
    "\n",
    "# Initial prompt. If `None`, the initial prompt will be generated automatically\n",
    "# PROMPT_TO_EVAL_FILE = None\n",
    "# PROMPT_TO_EVAL_FILE = \"_scored_100/writing-style-01-gpt-turbo-3.5-temp-0.3.md\"\n",
    "\n",
    "# Maximum number of rows to use from the dataset for initial prompt generation\n",
    "ROWS_INITIAL = 4\n",
    "# Maximum number of rows in each chunk\n",
    "ROWS_MAX = 1\n",
    "# Number of rows to use as `incorrect` examples\n",
    "ROWS_INCORRECT = 2\n",
    "\n",
    "\n",
    "# Use Few or Zero Shot?\n",
    "IS_FEW_SHOT = True\n",
    "EVAL_CONCURRENCY = 10\n",
    "\n",
    "\n",
    "# Stopping criteria (inclusive)\n",
    "GOAL_ACCURACY = 80\n",
    "MAX_ATTEMPTS_PER_PLAN = 5 # including the initial attempt before using UPDATEP\n",
    "\n",
    "\n",
    "# Model configurations\n",
    "# MODEL_PROMPT_WRITER_NAME = \"gpt-4-1106-preview\"\n",
    "MODEL_PROMPT_WRITER_NAME = \"gpt-3.5-turbo-16k\"\n",
    "# MODEL_PROMPT_WRITER_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "MODEL_PROMPT_WRITER_TEMPERATURE = 0.7\n",
    "MODEL_PROMPT_WRITER_MAX_TOKENS = 2400\n",
    "\n",
    "# MODEL_EVALUATE_NAME = \"gpt-3.5-turbo\"\n",
    "MODEL_EVALUATE_NAME = \"gpt-3.5-turbo-16k\"\n",
    "# MODEL_EVALUATE_NAME = \"gpt-4-1106-preview\"\n",
    "# MODEL_EVALUATE_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# MODEL_EVALUATE_NAME = \"togethercomputer/llama-2-70b-chat\"\n",
    "MODEL_EVALUATE_TEMPERATURE = 0.1\n",
    "MODEL_EVALUATE_MAX_TOKENS = 2200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling LLM cache...\n",
      "Loading ChatOpenAI model: gpt-3.5-turbo-16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kay/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChatOpenAI model: gpt-3.5-turbo-16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kay/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Set up LangChain models\n",
    "\n",
    "# if both model names start with `gpt-`, set cache\n",
    "if MODEL_PROMPT_WRITER_NAME.startswith(\"gpt-\") and MODEL_EVALUATE_NAME.startswith(\n",
    "    \"gpt-\"\n",
    "):\n",
    "    print(\"Enabling LLM cache...\")\n",
    "    set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "\n",
    "# Setup the prompt writer model\n",
    "model_prompt_writer = load_model(\n",
    "    MODEL_PROMPT_WRITER_NAME,\n",
    "    MODEL_PROMPT_WRITER_TEMPERATURE,\n",
    "    MODEL_PROMPT_WRITER_MAX_TOKENS,\n",
    ")\n",
    "\n",
    "# Setup the evaluation model\n",
    "model_evaluate = load_model(\n",
    "    MODEL_EVALUATE_NAME,\n",
    "    MODEL_EVALUATE_TEMPERATURE,\n",
    "    MODEL_EVALUATE_MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_NO</th>\n",
       "      <th>INPUT_TEXT_1</th>\n",
       "      <th>INPUT_TEXT_2</th>\n",
       "      <th>OUTPUT_Is_Same_Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td># The Ultimate Travel Nurse Salary Guide: 4 Mi...</td>\n",
       "      <td># The Best Travel Nursing Companies 2022\\n\\nTh...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>## How To Find the Highest Paying Travel Nursi...</td>\n",
       "      <td>## Highest Paying States for Travel Nurses\\n\\n...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>## Is Travel Nursing Worth It?\\n\\nIf you're lo...</td>\n",
       "      <td>### Increase in Best Travel Nursing Agency Lis...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>#### Internal Staff and Travel Nursing Reviews...</td>\n",
       "      <td>## Why You Should Consider ALL The Best Travel...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ROW_NO                                       INPUT_TEXT_1  \\\n",
       "0       1  # The Ultimate Travel Nurse Salary Guide: 4 Mi...   \n",
       "1       2  ## How To Find the Highest Paying Travel Nursi...   \n",
       "2       3  ## Is Travel Nursing Worth It?\\n\\nIf you're lo...   \n",
       "3       4  #### Internal Staff and Travel Nursing Reviews...   \n",
       "\n",
       "                                        INPUT_TEXT_2 OUTPUT_Is_Same_Author  \n",
       "0  # The Best Travel Nursing Companies 2022\\n\\nTh...                    NO  \n",
       "1  ## Highest Paying States for Travel Nurses\\n\\n...                   YES  \n",
       "2  ### Increase in Best Travel Nursing Agency Lis...                    NO  \n",
       "3  ## Why You Should Consider ALL The Best Travel...                   YES  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create it if it does not exist, or empty ./_log directory if it does\n",
    "if not os.path.exists(\"_log\"):\n",
    "    os.mkdir(\"_log\")\n",
    "else:\n",
    "    for filename in os.listdir(\"_log\"):\n",
    "        os.remove(os.path.join(\"_log\", filename))\n",
    "\n",
    "# Load the dataset\n",
    "df_all = load_and_clean_dataset(DATASET_FILE)\n",
    "df_all.head(ROWS_INITIAL)\n",
    "\n",
    "from utils_xml_table import slugify_column_names\n",
    "slugify_column_names(df_all).head(ROWS_INITIAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Auto Prompt Main Loop + Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 ranked ToT prompt construction plans...\n",
      "\n",
      "-> Running plan 6 with mutation 1...\n",
      "\n",
      "==============\n",
      "==============\n",
      "\n",
      "Plan 6:\n",
      "Compare the two pieces of text. Your OUTPUT MUST ONLY take the writing style, voice, tone and stucture into consideration. Do not consider the meaning or thematic similarity of the texts. Combine the approaches of content analysis, language modeling, literary analysis, stylistic analysis, and language processing. Analyze the texts for similarities and differences in writing style, voice, tone, and structure. Consider the use of language, narrative techniques, literary devices, and linguistic features. Apply machine learning algorithms to compare feature representations and generate text samples. This integrated strategy should provide a comprehensive assessment of whether the texts share a common author or not. \n",
      "\n",
      "Generating initial prompt...\n",
      "Getting chunk 1 retry 0 with 1 rows...\n",
      "Getting chunk 2 retry 0 with 1 rows...\n",
      "Getting chunk 3 retry 0 with 1 rows...\n",
      "Getting chunk 4 retry 0 with 1 rows...\n",
      "Getting chunk 5 retry 0 with 1 rows...\n",
      "Getting chunk 6 retry 0 with 1 rows...\n",
      "Getting chunk 7 retry 0 with 1 rows...\n",
      "Getting chunk 8 retry 0 with 1 rows...\n",
      "Getting chunk 9 retry 0 with 1 rows...\n",
      "Getting chunk 10 retry 0 with 1 rows...Getting chunk 11 retry 0 with 1 rows...\n",
      "\n",
      "Getting chunk 12 retry 0 with 1 rows...\n",
      "Getting chunk 13 retry 0 with 1 rows...\n",
      "Getting chunk 14 retry 0 with 1 rows...\n",
      "Getting chunk 15 retry 0 with 1 rows...\n",
      "Getting chunk 16 retry 0 with 1 rows...\n",
      "Getting chunk 17 retry 0 with 1 rows...\n",
      "Getting chunk 18 retry 0 with 1 rows...\n",
      "Getting chunk 19 retry 0 with 1 rows...\n",
      "Getting chunk 20 retry 0 with 1 rows...\n",
      "Correct answers: 60.00%\n",
      "Incorrect answers count: 8\n",
      "Pick 2 random incorrect examples...\n",
      "\n",
      "Updating prompt...\n",
      "Getting chunk 1 retry 0 with 1 rows...\n",
      "Getting chunk 2 retry 0 with 1 rows...\n",
      "Getting chunk 3 retry 0 with 1 rows...\n",
      "Getting chunk 4 retry 0 with 1 rows...\n",
      "Getting chunk 5 retry 0 with 1 rows...\n",
      "Getting chunk 6 retry 0 with 1 rows...\n",
      "Getting chunk 7 retry 0 with 1 rows...\n",
      "Getting chunk 8 retry 0 with 1 rows...\n",
      "Getting chunk 9 retry 0 with 1 rows...\n",
      "Getting chunk 10 retry 0 with 1 rows...\n",
      "Getting chunk 11 retry 0 with 1 rows...\n",
      "Getting chunk 12 retry 0 with 1 rows...\n",
      "Getting chunk 13 retry 0 with 1 rows...Getting chunk 14 retry 0 with 1 rows...\n",
      "\n",
      "Getting chunk 15 retry 0 with 1 rows...\n",
      "Getting chunk 16 retry 0 with 1 rows...\n",
      "Getting chunk 17 retry 0 with 1 rows...\n",
      "Getting chunk 18 retry 0 with 1 rows...\n",
      "Getting chunk 19 retry 0 with 1 rows...\n",
      "Getting chunk 20 retry 0 with 1 rows...\n",
      "Correct answers: 60.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 60.00% accuracy (8 wrong out of 20 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 60.00% accuracy (8 wrong out of 20 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Simplified and clarified the instructions and examples.\n",
      "- Removed unnecessary details and repetition.\n",
      "---\n",
      "\n",
      ">>> Accuracy below or same as previous accuracy for plan 6, mutation 1 is 60%. Next!\n",
      "\n",
      "-> Running plan 5 with mutation 1...\n",
      "\n",
      "==============\n",
      "==============\n",
      "\n",
      "Plan 5:\n",
      "Compare the two pieces of text. Your OUTPUT MUST ONLY take the writing style, voice, tone and stucture into consideration. Do not consider the meaning or thematic similarity of the texts. Utilize natural language processing techniques to analyze the linguistic features of the texts. Extract features such as word frequencies, syntactic structures, and semantic patterns. Apply machine learning algorithms to compare the feature representations of the texts and determine if they are more likely to have the same author or different authors. \n",
      "\n",
      "Generating initial prompt...\n",
      "Getting chunk 1 retry 0 with 1 rows...\n",
      "Getting chunk 2 retry 0 with 1 rows...\n",
      "Getting chunk 3 retry 0 with 1 rows...\n",
      "Getting chunk 4 retry 0 with 1 rows...\n",
      "Getting chunk 5 retry 0 with 1 rows...\n",
      "Getting chunk 6 retry 0 with 1 rows...\n",
      "Getting chunk 7 retry 0 with 1 rows...\n",
      "Getting chunk 8 retry 0 with 1 rows...\n",
      "Getting chunk 9 retry 0 with 1 rows...\n",
      "Getting chunk 10 retry 0 with 1 rows...\n",
      "Getting chunk 11 retry 0 with 1 rows...\n",
      "Getting chunk 12 retry 0 with 1 rows...\n",
      "Getting chunk 13 retry 0 with 1 rows...\n",
      "Getting chunk 14 retry 0 with 1 rows...\n",
      "Getting chunk 15 retry 0 with 1 rows...\n",
      "Getting chunk 16 retry 0 with 1 rows...\n",
      "Getting chunk 17 retry 0 with 1 rows...\n",
      "Getting chunk 18 retry 0 with 1 rows...\n",
      "Getting chunk 19 retry 0 with 1 rows...\n",
      "Getting chunk 20 retry 0 with 1 rows...\n",
      "Correct answers: 60.00%\n",
      "Incorrect answers count: 8\n",
      "Pick 2 random incorrect examples...\n",
      "\n",
      "Updating prompt...\n",
      "Error updating prompt: Prompt does not contain the variable %%%INPUT_TABLE%%%. Cannot replace the variable.\n",
      "Retrying... (1/2)\n",
      "Incorrect answers count: 8\n",
      "Pick 2 random incorrect examples...\n",
      "\n",
      "Updating prompt...\n",
      ">> Total cost: 0.024 USD, tokens used 7274\n",
      "Getting chunk 1 retry 0 with 1 rows...\n",
      "Getting chunk 2 retry 0 with 1 rows...\n",
      "Getting chunk 3 retry 0 with 1 rows...\n",
      "Getting chunk 4 retry 0 with 1 rows...\n",
      "Getting chunk 5 retry 0 with 1 rows...\n",
      "Getting chunk 6 retry 0 with 1 rows...\n",
      "Getting chunk 7 retry 0 with 1 rows...\n",
      "Getting chunk 8 retry 0 with 1 rows...\n",
      "Getting chunk 9 retry 0 with 1 rows...\n",
      "Getting chunk 10 retry 0 with 1 rows...\n",
      "Getting chunk 11 retry 0 with 1 rows...\n",
      "Getting chunk 12 retry 0 with 1 rows...\n",
      "Getting chunk 13 retry 0 with 1 rows...\n",
      "Getting chunk 14 retry 0 with 1 rows...\n",
      "Getting chunk 15 retry 0 with 1 rows...\n",
      "Getting chunk 16 retry 0 with 1 rows...\n",
      "Getting chunk 17 retry 0 with 1 rows...\n",
      "Getting chunk 18 retry 0 with 1 rows...\n",
      "Getting chunk 19 retry 0 with 1 rows...\n",
      "Getting chunk 20 retry 0 with 1 rows...\n",
      "Correct answers: 60.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 60.00% accuracy (8 wrong out of 20 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 60.00% accuracy (8 wrong out of 20 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Updated the instructions and examples to provide clearer guidance on analyzing the writing style, voice, tone, and structure of the texts.\n",
      "- Emphasized the extraction of features such as word frequencies, syntactic structures, and semantic patterns.\n",
      "- Added step 5 to consider contextual information and background knowledge.\n",
      "- Added step 6 to apply machine learning algorithms to enhance the accuracy of the classification.\n",
      "---\n",
      "\n",
      ">>> Accuracy below or same as previous accuracy for plan 5, mutation 1 is 60%. Next!\n",
      "\n",
      "-> Running plan 4 with mutation 1...\n",
      "\n",
      "==============\n",
      "==============\n",
      "\n",
      "Plan 4:\n",
      "Compare the two pieces of text. Your OUTPUT MUST ONLY take the writing style, voice, tone and stucture into consideration. Do not consider the meaning or thematic similarity of the texts. Focus on the stylistic aspects of the texts, such as choice of vocabulary, sentence length, figurative language, and use of rhetorical devices. Compare these elements in both texts and assess whether they exhibit similar patterns that indicate a common author or distinct patterns that suggest different authors. \n",
      "\n",
      "Generating initial prompt...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m auto_prompt_strategy \u001b[39m=\u001b[39m TreeSearchStrategy(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_prompt_writer\u001b[39m=\u001b[39mmodel_prompt_writer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model_evaluate\u001b[39m=\u001b[39mmodel_evaluate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     is_use_eval_aware_dataset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Execute the strategy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# prompt_str, accuracy, plan = auto_prompt_strategy.run()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m prompt_str, accuracy, plan \u001b[39m=\u001b[39m auto_prompt_strategy\u001b[39m.\u001b[39;49mrun(max_mutations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, min_acceptable_accuracy\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Save the final prompt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kay/devel/auto-prompt-builder/Auto-Prompter.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m save_log_file(\u001b[39m\"\u001b[39m\u001b[39m10-prompt_final.md\u001b[39m\u001b[39m\"\u001b[39m, prompt_str)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/strategies/tree_search.py:146\u001b[0m, in \u001b[0;36mTreeSearchStrategy.run\u001b[0;34m(self, max_mutations, min_acceptable_accuracy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m mutation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_mutations \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    144\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m-> Running plan \u001b[39m\u001b[39m{\u001b[39;00mplan\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m with mutation \u001b[39m\u001b[39m{\u001b[39;00mmutation\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m     accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_one_mutation(\n\u001b[1;32m    147\u001b[0m         plan, mutation, min_acceptable_accuracy\n\u001b[1;32m    148\u001b[0m     )\n\u001b[1;32m    150\u001b[0m     \u001b[39m# Break the loop if the goal accuracy is achieved\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_goal_reached(accuracy, plan, mutation):\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/strategies/tree_search.py:108\u001b[0m, in \u001b[0;36mTreeSearchStrategy._run_one_mutation\u001b[0;34m(self, plan, mutation, min_acceptable_accuracy)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_one_mutation\u001b[39m(\u001b[39mself\u001b[39m, plan, mutation\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, min_acceptable_accuracy\u001b[39m=\u001b[39m\u001b[39m70\u001b[39m):\n\u001b[1;32m     98\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    Runs a single mutation on the plan and returns the accuracy.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m        float: The accuracy after applying the mutation, and updating the prompt.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     accuracy, prompt_str, df_generated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_plan_initial_prompt(\n\u001b[1;32m    109\u001b[0m         plan, mutation\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_must_stop(\n\u001b[1;32m    113\u001b[0m         accuracy, min_acceptable_accuracy, plan, mutation\n\u001b[1;32m    114\u001b[0m     ) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_goal_reached(accuracy, plan, mutation):\n\u001b[1;32m    115\u001b[0m         \u001b[39mreturn\u001b[39;00m accuracy\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/strategies/base.py:257\u001b[0m, in \u001b[0;36mBaseStrategy._run_plan_initial_prompt\u001b[0;34m(self, plan, mutation)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m==============\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m==============\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mPlan \u001b[39m\u001b[39m{\u001b[39;00mplan\u001b[39m.\u001b[39mid\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mplan_text\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    256\u001b[0m \u001b[39m# Generate the initial prompt for this plan\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m prompt_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_initial_prompt(plan)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Evaluate the initial prompt\u001b[39;00m\n\u001b[1;32m    260\u001b[0m df_generated, accuracy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluate_prompt(prompt_str, plan\u001b[39m.\u001b[39mid)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/strategies/base.py:124\u001b[0m, in \u001b[0;36mBaseStrategy._generate_initial_prompt\u001b[0;34m(self, plan)\u001b[0m\n\u001b[1;32m    114\u001b[0m df_sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_mistake_tracker\u001b[39m.\u001b[39mget_sample(\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrows_initial, is_include_mistakes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_use_eval_aware_dataset\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    117\u001b[0m gen_prompt_initial \u001b[39m=\u001b[39m GeneratePromptInitial(\n\u001b[1;32m    118\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_prompt_writer,\n\u001b[1;32m    119\u001b[0m     is_few_shot\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_few_shot,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m     plan_id\u001b[39m=\u001b[39mplan\u001b[39m.\u001b[39mid,\n\u001b[1;32m    123\u001b[0m )\n\u001b[0;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m gen_prompt_initial\u001b[39m.\u001b[39;49minvoke()\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/generate_prompt_initial.py:68\u001b[0m, in \u001b[0;36mGeneratePromptInitial.invoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mwith\u001b[39;00m get_openai_callback() \u001b[39mas\u001b[39;00m cb:\n\u001b[0;32m---> 68\u001b[0m     answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chain()\u001b[39m.\u001b[39;49minvoke(variables)\n\u001b[1;32m     69\u001b[0m     print_cost(cb)\n\u001b[1;32m     71\u001b[0m save_log_file(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile_prefix\u001b[39m}\u001b[39;00m\u001b[39m-(2)-response.md\u001b[39m\u001b[39m\"\u001b[39m, answer)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1762\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1761\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1762\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1763\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1764\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1765\u001b[0m             patch_config(\n\u001b[1;32m   1766\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1767\u001b[0m             ),\n\u001b[1;32m   1768\u001b[0m         )\n\u001b[1;32m   1769\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:165\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    156\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m    162\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[1;32m    164\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 165\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    166\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    167\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    168\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    169\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    170\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    171\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    172\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    173\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[1;32m    174\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:543\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    537\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    541\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    542\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 543\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:407\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[1;32m    406\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e, response\u001b[39m=\u001b[39mLLMResult(generations\u001b[39m=\u001b[39m[]))\n\u001b[0;32m--> 407\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[1;32m    409\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[1;32m    410\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[1;32m    411\u001b[0m ]\n\u001b[1;32m    412\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:397\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[1;32m    395\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 397\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[1;32m    398\u001b[0m                 m,\n\u001b[1;32m    399\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    400\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    401\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    402\u001b[0m             )\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:589\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 589\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    590\u001b[0m             messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    591\u001b[0m         )\n\u001b[1;32m    592\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:437\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m message_dicts, params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    432\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[1;32m    433\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    434\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m: stream} \u001b[39mif\u001b[39;00m stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}),\n\u001b[1;32m    435\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    436\u001b[0m }\n\u001b[0;32m--> 437\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompletion_with_retry(\n\u001b[1;32m    438\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    439\u001b[0m )\n\u001b[1;32m    440\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/langchain_community/chat_models/openai.py:354\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    356\u001b[0m retry_decorator \u001b[39m=\u001b[39m _create_retry_decorator(\u001b[39mself\u001b[39m, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[1;32m    358\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:272\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 272\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    598\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    644\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 645\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    647\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    648\u001b[0m             {\n\u001b[1;32m    649\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    650\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    651\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    652\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    653\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    654\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    655\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    656\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    657\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    658\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    659\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    660\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    661\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    662\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    663\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    664\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    665\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    666\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    669\u001b[0m             },\n\u001b[1;32m    670\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    671\u001b[0m         ),\n\u001b[1;32m    672\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    673\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    674\u001b[0m         ),\n\u001b[1;32m    675\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    676\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    677\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    678\u001b[0m     )\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    855\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    856\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    859\u001b[0m     )\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    876\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    878\u001b[0m         request,\n\u001b[1;32m    879\u001b[0m         auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth,\n\u001b[1;32m    880\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    881\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    883\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    916\u001b[0m     request,\n\u001b[1;32m    917\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    918\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    919\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    944\u001b[0m         request,\n\u001b[1;32m    945\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    946\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1018\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/devel/auto-prompt-builder/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1292\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1290\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1291\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1293\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1165\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1166\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1167\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup the strategy executor\n",
    "# Options so far: SequentialStrategy, BasicStrategy, TreeSearchStrategy\n",
    "auto_prompt_strategy = TreeSearchStrategy(\n",
    "    model_prompt_writer=model_prompt_writer,\n",
    "    model_evaluate=model_evaluate,\n",
    "    df_original=df_all,\n",
    "    idea_seed=IDEA_SEED,\n",
    "    goal_accuracy=GOAL_ACCURACY,\n",
    "    max_attempts_per_plan=MAX_ATTEMPTS_PER_PLAN,\n",
    "    is_few_shot=IS_FEW_SHOT,\n",
    "    eval_concurrency=EVAL_CONCURRENCY,\n",
    "    rows_initial=ROWS_INITIAL,\n",
    "    rows_max=ROWS_MAX,\n",
    "    rows_incorrect=ROWS_INCORRECT,\n",
    "    is_use_eval_aware_dataset=True,\n",
    ")\n",
    "\n",
    "# Execute the strategy\n",
    "# prompt_str, accuracy, plan = auto_prompt_strategy.run()\n",
    "prompt_str, accuracy, plan = auto_prompt_strategy.run(max_mutations=1, min_acceptable_accuracy=50)\n",
    "\n",
    "# Save the final prompt\n",
    "save_log_file(\"10-prompt_final.md\", prompt_str)\n",
    "print(f\"\\nFinal prompt saved with accuracy {accuracy:.2f}%\")\n",
    "if plan is not None:\n",
    "    print(f\"Plan {plan.id}: {plan.expert_title}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
