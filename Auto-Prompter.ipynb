{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPrompt - Auto Write Evaluation Prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to know what the right prompt is, and it's hard to know if you've found it. This project aims to automate the process of finding the perfect evaluation prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from utils import save_tmp_file, load_model\n",
    "from data_handling import load_and_clean_dataset\n",
    "from eval import invoke_test_prompt_against_dataset\n",
    "from prompt_initial import invoke_generate_prompt_initial\n",
    "from prompt_update import (\n",
    "    invoke_update_prompt_with_retry,\n",
    "    previous_attempts_add,\n",
    "    previous_attempts_to_str,\n",
    ")\n",
    "\n",
    "from prompts.writep_few_shot.prompt import (\n",
    "    get_prompt_template as get_few_shot_prompt_template,\n",
    ")\n",
    "from prompts.writep_zero_shot.prompt import (\n",
    "    get_prompt_template as get_zero_shot_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = \"./datasets/sentiment_analysis_examples_25.csv\"\n",
    "# DATASET_FILE = \"./datasets/dataset-writing-style-v-not-v.xlsx\"\n",
    "# DATASET_FILE = \"./datasets/writing-style.xlsx\"\n",
    "\n",
    "# Seed Idea for prompt generation\n",
    "IDEA_SEED = \"\"\"Decide the sentiment of the input text.\"\"\"\n",
    "# IDEA_SEED = \"\"\"Compare the writing style of the two pieces of text. Your OUTPUT MUST ONLY take the writing style into consideration, NOT the meaning or thematic similarity of the texts.\"\"\".strip()\n",
    "\n",
    "\n",
    "# Initial prompt. If `None`, the initial prompt will be generated automatically\n",
    "PROMPT_TO_EVAL_FILE = None\n",
    "# PROMPT_TO_EVAL_FILE = \"./prompt-v-or-not-v-01.md\"\n",
    "# PROMPT_TO_EVAL_FILE = \"./_scored_100/sentiment-05-zero-shot.md\"\n",
    "\n",
    "# Maximum number of rows to use from the dataset for initial prompt generation\n",
    "ROWS_INITIAL = 4\n",
    "# Maximum number of rows in each chunk\n",
    "ROWS_MAX = 10\n",
    "# Number of rows to use as `incorrect` examples\n",
    "ROWS_INCORRECT = 5\n",
    "\n",
    "\n",
    "# Prompt file paths\n",
    "PROMPT_UPDATE_FILE = \"./prompts/PROMPT_UPDATEP.json\"\n",
    "IS_FEW_SHOT = False\n",
    "\n",
    "# Model configurations\n",
    "MODEL_PROMPT_WRITER_NAME = \"gpt-4-1106-preview\"\n",
    "# MODEL_PROMPT_WRITER_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_PROMPT_WRITER_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "MODEL_PROMPT_WRITER_TEMPERATURE = 0.7\n",
    "MODEL_PROMPT_WRITER_MAX_TOKENS = 2000\n",
    "\n",
    "MODEL_EVALUATE_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_EVALUATE_NAME = \"gpt-4-1106-preview\"\n",
    "# MODEL_EVALUATE_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# MODEL_EVALUATE_NAME = \"togethercomputer/llama-2-70b-chat\"\n",
    "MODEL_EVALUATE_TEMPERATURE = 0.3\n",
    "MODEL_EVALUATE_MAX_TOKENS = 800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LangChain models\n",
    "\n",
    "# if both model names start with `gpt-`, set cache\n",
    "if MODEL_PROMPT_WRITER_NAME.startswith(\"gpt-\") and MODEL_EVALUATE_NAME.startswith(\n",
    "    \"gpt-\"\n",
    "):\n",
    "    print(\"Enabling LLM cache...\")\n",
    "    set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "\n",
    "# Setup the prompt writer model\n",
    "model_prompt_writer = load_model(\n",
    "    MODEL_PROMPT_WRITER_NAME,\n",
    "    MODEL_PROMPT_WRITER_TEMPERATURE,\n",
    "    MODEL_PROMPT_WRITER_MAX_TOKENS,\n",
    ")\n",
    "\n",
    "# Setup the evaluation model\n",
    "model_evaluate = load_model(\n",
    "    MODEL_EVALUATE_NAME,\n",
    "    MODEL_EVALUATE_TEMPERATURE,\n",
    "    MODEL_EVALUATE_MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty ./_tmp directory\n",
    "for filename in os.listdir(\"_tmp\"):\n",
    "    os.remove(os.path.join(\"_tmp\", filename))\n",
    "\n",
    "# Load the dataset\n",
    "df_all = load_and_clean_dataset(DATASET_FILE)\n",
    "\n",
    "# If df_all has more rows than ROWS_INITIAL, take the first ROWS_INITIAL rows\n",
    "df_sample = df_all\n",
    "if len(df_all) > ROWS_INITIAL:\n",
    "    df_sample = df_all.head(ROWS_INITIAL)\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Initial Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If PROMPT_TO_EVAL_FILE is not None, load the prompt from the file\n",
    "prompt_str = \"\"\n",
    "if PROMPT_TO_EVAL_FILE is not None:\n",
    "    print(f\"Loading prompt from {PROMPT_TO_EVAL_FILE}\")\n",
    "    with open(PROMPT_TO_EVAL_FILE, \"r\") as f:\n",
    "        prompt_str = f.read()\n",
    "else:\n",
    "    # Generate the initial prompt\n",
    "    prompt_init_template = (\n",
    "        get_few_shot_prompt_template()\n",
    "        if IS_FEW_SHOT\n",
    "        else get_zero_shot_prompt_template()\n",
    "    )\n",
    "    prompt_str = invoke_generate_prompt_initial(\n",
    "        model_prompt_writer, prompt_init_template, df_sample, IDEA_SEED\n",
    "    )\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_str)\n",
    "# print(prompt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Initial Prompt against the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompt counter used for the main loop\n",
    "i_prompt = 1\n",
    "\n",
    "df_generated, accuracy = invoke_test_prompt_against_dataset(\n",
    "    prompt, df_all, model_evaluate, i_prompt, ROWS_MAX, concurrency=5\n",
    ")\n",
    "\n",
    "df_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Main loop to auto-magically improve the prompt\n",
    "\n",
    "The main loop will run until the prompt is good enough (or max loops is reached).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_attempts = []\n",
    "previous_attempts_add(previous_attempts, i_prompt, accuracy, \"First attempt.\")\n",
    "\n",
    "\n",
    "# Loop until accuracy is greater than 95% or 5 iterations have been reached\n",
    "while accuracy < 95 and i_prompt < 10:\n",
    "    i_prompt = i_prompt + 1\n",
    "\n",
    "    previous_attempts_str = previous_attempts_to_str(previous_attempts, df_all)\n",
    "    print(f\"Previous attempts:\\n{previous_attempts_str}\\n\\n\")\n",
    "\n",
    "    prompt_previous = prompt_str\n",
    "    prompt_template_updatep = load_prompt(PROMPT_UPDATE_FILE)\n",
    "\n",
    "    prompt_str, changes_made_str = invoke_update_prompt_with_retry(\n",
    "        prompt_template_updatep,\n",
    "        df_generated,\n",
    "        prompt_previous,\n",
    "        model_prompt_writer,\n",
    "        previous_attempts_str,\n",
    "        i_prompt,\n",
    "        ROWS_INCORRECT,\n",
    "        IDEA_SEED,\n",
    "    )\n",
    "\n",
    "    prompt_updated = PromptTemplate.from_template(prompt_str)\n",
    "    df_generated, accuracy = invoke_test_prompt_against_dataset(\n",
    "        prompt_updated, df_all, model_evaluate, i_prompt, ROWS_MAX, concurrency=5\n",
    "    )\n",
    "\n",
    "    previous_attempts_add(previous_attempts, i_prompt, accuracy, changes_made_str)\n",
    "\n",
    "# print(f\"\\n\\nFinal prompt:\\n{prompt_generated_str}\")\n",
    "save_tmp_file(\"10-prompt_final.md\", prompt_str)\n",
    "print(f\"\\nFinal prompt saved with accuracy {accuracy:.2f}%\")\n",
    "\n",
    "# print(json.dumps(previous_attempts, indent=2))\n",
    "print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "print(previous_attempts_to_str(previous_attempts, df_all))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
