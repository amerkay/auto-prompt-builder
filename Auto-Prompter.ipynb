{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoPrompt - Auto Write Evaluation Prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Idea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to know what the right prompt is, and it's hard to know if you've found it. This project aims to automate the process of finding the perfect evaluation prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "from utils import save_tmp_file, load_model\n",
    "from data_handling import load_and_clean_dataset\n",
    "from evaluate_against_dataset import EvaluateAgainstDataset\n",
    "from generate_prompt_initial import GeneratePromptInitial\n",
    "from generate_prompt_update import GeneratePromptUpdate\n",
    "from generate_expert_plans import GenerateExpertPlans\n",
    "from previous_attempts import PreviousAttempts, Attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = \"./datasets/sentiment_analysis_examples_25.csv\"\n",
    "# DATASET_FILE = \"./datasets/dataset-writing-style-v-not-v.xlsx\"\n",
    "# DATASET_FILE = \"./datasets/writing-style.xlsx\"\n",
    "# DATASET_FILE = \"./datasets/writing-style-30-100-words.xlsx\"\n",
    "\n",
    "# Seed Idea for prompt generation\n",
    "IDEA_SEED = \"\"\"Decide the sentiment of the input text.\"\"\"\n",
    "# IDEA_SEED = \"\"\"Compare the writing style of the two pieces of text. Your OUTPUT MUST ONLY take the writing style into consideration, NOT the meaning or thematic similarity of the texts.\"\"\".strip()\n",
    "\n",
    "\n",
    "# Initial prompt. If `None`, the initial prompt will be generated automatically\n",
    "# PROMPT_TO_EVAL_FILE = None\n",
    "# PROMPT_TO_EVAL_FILE = \"_scored_100/writing-style-01-gpt-turbo-3.5-temp-0.3.md\"\n",
    "\n",
    "# Maximum number of rows to use from the dataset for initial prompt generation\n",
    "ROWS_INITIAL = 8\n",
    "# Maximum number of rows in each chunk\n",
    "ROWS_MAX = 13\n",
    "# Number of rows to use as `incorrect` examples\n",
    "ROWS_INCORRECT = 5\n",
    "\n",
    "\n",
    "# Use Few or Zero Shot?\n",
    "IS_FEW_SHOT = True\n",
    "EVAL_CONCURRENCY = 10\n",
    "\n",
    "\n",
    "# Stopping criteria (inclusive)\n",
    "GOAL_ACCURACY = 98\n",
    "MAX_ATTEMPTS_PER_PLAN = 2\n",
    "\n",
    "\n",
    "# Model configurations\n",
    "MODEL_PROMPT_WRITER_NAME = \"gpt-4-1106-preview\"\n",
    "# MODEL_PROMPT_WRITER_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_PROMPT_WRITER_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "MODEL_PROMPT_WRITER_TEMPERATURE = 0.6\n",
    "MODEL_PROMPT_WRITER_MAX_TOKENS = 2000\n",
    "\n",
    "MODEL_EVALUATE_NAME = \"gpt-3.5-turbo\"\n",
    "# MODEL_EVALUATE_NAME = \"gpt-4-1106-preview\"\n",
    "# MODEL_EVALUATE_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# MODEL_EVALUATE_NAME = \"togethercomputer/llama-2-70b-chat\"\n",
    "MODEL_EVALUATE_TEMPERATURE = 0.01\n",
    "MODEL_EVALUATE_MAX_TOKENS = 1400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling LLM cache...\n",
      "Loading ChatOpenAI model: gpt-4-1106-preview\n",
      "Loading ChatOpenAI model: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "# Set up LangChain models\n",
    "\n",
    "# if both model names start with `gpt-`, set cache\n",
    "if MODEL_PROMPT_WRITER_NAME.startswith(\"gpt-\") and MODEL_EVALUATE_NAME.startswith(\n",
    "    \"gpt-\"\n",
    "):\n",
    "    print(\"Enabling LLM cache...\")\n",
    "    set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "\n",
    "# Setup the prompt writer model\n",
    "model_prompt_writer = load_model(\n",
    "    MODEL_PROMPT_WRITER_NAME,\n",
    "    MODEL_PROMPT_WRITER_TEMPERATURE,\n",
    "    MODEL_PROMPT_WRITER_MAX_TOKENS,\n",
    ")\n",
    "\n",
    "# Setup the evaluation model\n",
    "model_evaluate = load_model(\n",
    "    MODEL_EVALUATE_NAME,\n",
    "    MODEL_EVALUATE_TEMPERATURE,\n",
    "    MODEL_EVALUATE_MAX_TOKENS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INPUT: Sentence</th>\n",
       "      <th>OUTPUT: Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this new phone</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is just okay. Nothing special. üòê</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unfortunately, it broke the first day I used it</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I guess it could've been worse üòÖ</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Waiting forever for a response... üòí</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The movie was both amazing and boring üòï</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Not sure if I liked it or not</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Absolutely fantastic experience!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   INPUT: Sentence OUTPUT: Sentiment\n",
       "0                            I love this new phone          positive\n",
       "1            This is just okay. Nothing special. üòê           neutral\n",
       "2  Unfortunately, it broke the first day I used it          negative\n",
       "3                 I guess it could've been worse üòÖ           neutral\n",
       "4              Waiting forever for a response... üòí          negative\n",
       "5          The movie was both amazing and boring üòï           neutral\n",
       "6                    Not sure if I liked it or not           neutral\n",
       "7                 Absolutely fantastic experience!          positive"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty ./_tmp directory\n",
    "for filename in os.listdir(\"_tmp\"):\n",
    "    os.remove(os.path.join(\"_tmp\", filename))\n",
    "\n",
    "# Load the dataset\n",
    "df_all = load_and_clean_dataset(DATASET_FILE)\n",
    "\n",
    "# If df_all has more rows than ROWS_INITIAL, take the first ROWS_INITIAL rows\n",
    "df_sample = df_all\n",
    "if len(df_all) > ROWS_INITIAL:\n",
    "    df_sample = df_all.head(ROWS_INITIAL)\n",
    "\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Initial Expert Ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 ranked ToT prompt construction plans...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Total cost: 0.028 USD, tokens used 1781\n",
      "\n",
      "=====================\n",
      "=====================\n",
      "\n",
      "Plan 6:\n",
      "Decide the sentiment of the input text. Integrate methods from sentiment analysis, natural language processing, machine learning, psycholinguistics, and social media analysis. Analyze the text for emotional language, utilize NLP to understand sentence structure and sentiment-bearing phrases, apply machine learning for pattern recognition, consider the psychological aspects of language use, and assess the tone and emotional impact as seen in social media contexts. This comprehensive approach should accurately classify the text's sentiment as positive, neutral, or negative. \n",
      "\n",
      "Generating initial prompt...\n",
      ">> Total cost: 0.034 USD, tokens used 2273\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 88.00%\n",
      "Incorrect answers count: 3\n",
      "Pick the first 3 incorrect examples...\n",
      "Updating prompt...\n",
      ">> Total cost: 0.047 USD, tokens used 2530\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 88.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 88.00% accuracy (3 wrong out of 25 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 88.00% accuracy (3 wrong out of 25 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Emphasized the importance of interpreting emojis in context with the rest of the sentence.\n",
      "- Added a point about considering the overall tone and context, including sarcasm or understatement.\n",
      "- Clarified that contradictory phrases should be weighed, and the sentiment is determined by the dominant tone.\n",
      "- Added two new examples (rows 4 and 5) to illustrate the analysis of sentences with neutral sentiment and those with mixed sentiments where one sentiment is dominant.\n",
      "---\n",
      "\n",
      "\n",
      "=====================\n",
      "=====================\n",
      "\n",
      "Plan 3:\n",
      "Decide the sentiment of the input text. Develop a machine learning model that classifies text into sentiment categories. Train the model using a dataset of labeled examples, including the use of emojis, to recognize patterns indicative of positive, neutral, or negative sentiments. \n",
      "\n",
      "Generating initial prompt...\n",
      ">> Total cost: 0.033 USD, tokens used 2191\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 96.00%\n",
      "Incorrect answers count: 1\n",
      "Pick the first 1 incorrect examples...\n",
      "Updating prompt...\n",
      ">> Total cost: 0.042 USD, tokens used 2255\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 92.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 96.00% accuracy (1 wrong out of 25 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 92.00% accuracy (2 wrong out of 25 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Added guidance on considering the overall impact of sentence elements when they point to different sentiments.\n",
      "- Included an additional example (ROW_NO: 4) in the EXAMPLE section to illustrate how to approach sentences with mixed sentiments but an overall negative impact.\n",
      "---\n",
      "\n",
      "\n",
      "=====================\n",
      "=====================\n",
      "\n",
      "Plan 2:\n",
      "Decide the sentiment of the input text. Use natural language processing techniques to parse the sentence structure and identify key sentiment-bearing phrases. Pay special attention to negations, intensifiers, and diminishers that can alter the sentiment of a statement. \n",
      "\n",
      "Generating initial prompt...\n",
      ">> Total cost: 0.034 USD, tokens used 2243\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 96.00%\n",
      "Incorrect answers count: 1\n",
      "Pick the first 1 incorrect examples...\n",
      "Updating prompt...\n",
      ">> Total cost: 0.038 USD, tokens used 2116\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 92.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 96.00% accuracy (1 wrong out of 25 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 92.00% accuracy (2 wrong out of 25 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Added an example to guide the model on how to interpret subtle negative sentiments.\n",
      "- Included a reminder statement regarding the format of the rows and field names as per the important note.\n",
      "---\n",
      "\n",
      "\n",
      "=====================\n",
      "=====================\n",
      "\n",
      "Plan 1:\n",
      "Decide the sentiment of the input text. Examine the text for emotional language, including adjectives, adverbs, and phrases commonly associated with positive, neutral, or negative sentiments. Note the presence of emojis and their typical emotional connotations to enhance the analysis. \n",
      "\n",
      "Generating initial prompt...\n",
      ">> Total cost: 0.037 USD, tokens used 2333\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 88.00%\n",
      "Incorrect answers count: 3\n",
      "Pick the first 3 incorrect examples...\n",
      "Updating prompt...\n",
      ">> Total cost: 0.051 USD, tokens used 2700\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 96.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 88.00% accuracy (3 wrong out of 25 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 96.00% accuracy (1 wrong out of 25 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Added instruction to consider the overall sentiment in cases of mixed sentiments.\n",
      "- Added instruction to recognize subtle negative cues as potentially negative.\n",
      "- Added an example with mixed sentiments (Example 4) to guide the model in cases where both positive and negative elements are present.\n",
      "- Added an example with a subtle negative cue (Example 5) to demonstrate how to interpret expressions of lack of enthusiasm or slight dissatisfaction.\n",
      "---\n",
      "\n",
      "\n",
      "=====================\n",
      "=====================\n",
      "\n",
      "Plan 5:\n",
      "Decide the sentiment of the input text. Assess the text as if it were a social media post, considering the informal language and emoji usage. Determine the sentiment by evaluating the overall tone and the emotional impact of the message on the reader. \n",
      "\n",
      "Generating initial prompt...\n",
      ">> Total cost: 0.035 USD, tokens used 2276\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 92.00%\n",
      "Incorrect answers count: 2\n",
      "Pick the first 2 incorrect examples...\n",
      "Updating prompt...\n",
      ">> Total cost: 0.046 USD, tokens used 2480\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 96.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 92.00% accuracy (2 wrong out of 25 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 96.00% accuracy (1 wrong out of 25 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Added an example (ROW_NO: 5) that has mixed sentiments with an explanation emphasizing that the negative aspect stands out, resulting in an overall negative sentiment. This addresses the incorrect answer for ROW_NO: 10.\n",
      "- Added an example (ROW_NO: 6) that shows a mild negative sentiment, which is meant to correct the model's previous neutral classification of a similar sentiment in ROW_NO: 22.\n",
      "---\n",
      "\n",
      "\n",
      "=====================\n",
      "=====================\n",
      "\n",
      "Plan 4:\n",
      "Decide the sentiment of the input text. Analyze the text from a psycholinguistic perspective to understand the emotional state of the speaker. Consider the psychological underpinnings of the words and phrases used, as well as the context implied by emojis. \n",
      "\n",
      "Generating initial prompt...\n",
      ">> Total cost: 0.035 USD, tokens used 2282\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 92.00%\n",
      "Incorrect answers count: 2\n",
      "Pick the first 2 incorrect examples...\n",
      "Updating prompt...\n",
      ">> Total cost: 0.046 USD, tokens used 2493\n",
      "Getting chunk 1 retry 0 with 13 rows...\n",
      "Getting chunk 2 retry 0 with 12 rows...\n",
      "Correct answers: 96.00%\n",
      "\n",
      "---\n",
      "### Attempt 1: 92.00% accuracy (2 wrong out of 25 test rows)\n",
      "First attempt.\n",
      "### Attempt 2: 96.00% accuracy (1 wrong out of 25 test rows)\n",
      "Changes made to the prompt compared to attempt 1:\n",
      "- Added an example with contrasting sentiments using the word \"but\" to emphasize that the sentiment after \"but\" often outweighs the sentiment before it.\n",
      "- Added an example with the word \"Meh\" to demonstrate that it expresses a negative sentiment.\n",
      "---\n",
      "\n",
      "\n",
      "Final prompt saved with accuracy 96.00%\n"
     ]
    }
   ],
   "source": [
    "# Generate the expert ToT plans\n",
    "gen_expert_plans = GenerateExpertPlans(\n",
    "    model=model_prompt_writer, df_sample=df_sample, idea_seed=IDEA_SEED\n",
    ")\n",
    "ranked_expert_plans = gen_expert_plans.invoke()\n",
    "# print(json.dumps(ranked_expert_plans, indent=2))\n",
    "\n",
    "\n",
    "# Create an instance of the EvalAgainstDataset class\n",
    "evaluator = EvaluateAgainstDataset(\n",
    "    model=model_evaluate,\n",
    "    df_original=df_all,\n",
    "    max_chunk_rows=ROWS_MAX,\n",
    "    concurrency=EVAL_CONCURRENCY,\n",
    ")\n",
    "\n",
    "# Init global variables\n",
    "previous_attempts, prompt_str, accuracy = None, None, None\n",
    "\n",
    "# Loop through the expert plans\n",
    "for i, plan in enumerate(ranked_expert_plans):\n",
    "    # if plan.id != 5:\n",
    "    #     continue\n",
    "    \n",
    "    # The prompt counter used for the main loop\n",
    "    attempt_no = 1\n",
    "\n",
    "    # The previous attempts list\n",
    "    previous_attempts = PreviousAttempts(df_all_length=len(df_all))\n",
    "\n",
    "    # the plan\n",
    "    plan_text = plan.to_string(idea_seed=IDEA_SEED)\n",
    "\n",
    "    print(\"\\n=====================\\n=====================\\n\")\n",
    "    print(f\"Plan {plan.id}:\")\n",
    "    print(plan_text, \"\\n\")\n",
    "\n",
    "    # Generate the initial prompt for this plan\n",
    "    gen_prompt_initial = GeneratePromptInitial(\n",
    "        model=model_prompt_writer,\n",
    "        is_few_shot=IS_FEW_SHOT,\n",
    "        df_sample=df_sample,\n",
    "        idea_seed=plan_text,\n",
    "        plan_id=plan.id,\n",
    "    )\n",
    "    prompt_str = gen_prompt_initial.invoke()\n",
    "\n",
    "    # Test the Initial Prompt against the dataset\n",
    "    df_generated, accuracy = evaluator.invoke(\n",
    "        prompt_str=prompt_str, plan_id=plan.id, attempt_no=attempt_no\n",
    "    )\n",
    "\n",
    "    previous_attempts.add(\n",
    "        Attempt(attempt_no=attempt_no, accuracy=accuracy, changes_made=\"First attempt.\")\n",
    "    )\n",
    "\n",
    "    ## The Main loop to auto-magically improve the prompt ###\n",
    "    ## Runs until the prompt is good enough (or max loops is reached).\n",
    "    while accuracy < GOAL_ACCURACY and attempt_no < MAX_ATTEMPTS_PER_PLAN:\n",
    "        attempt_no = attempt_no + 1\n",
    "        \n",
    "        # Generate the updated prompt for this plan\n",
    "        gen_prompt_update = GeneratePromptUpdate(\n",
    "            model=model_prompt_writer,\n",
    "            attempt_no=attempt_no,\n",
    "            plan_id=plan.id,\n",
    "            idea_seed=plan_text,\n",
    "            previous_attempts=previous_attempts,\n",
    "            max_rows_incorrect=ROWS_INCORRECT,\n",
    "        )\n",
    "        prompt_str, changes_made_str = gen_prompt_update.invoke_with_retry(\n",
    "            df_generated=df_generated,\n",
    "            prompt_previous=prompt_str,\n",
    "        )\n",
    "\n",
    "        # Test the Updated Prompt against the dataset\n",
    "        df_generated, accuracy = evaluator.invoke(\n",
    "            prompt_str=prompt_str, plan_id=plan.id, attempt_no=attempt_no\n",
    "        )\n",
    "\n",
    "        previous_attempts.add(\n",
    "            Attempt(\n",
    "                attempt_no=attempt_no, accuracy=accuracy, changes_made=changes_made_str\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # print(json.dumps(previous_attempts, indent=2))\n",
    "        print(\"\\n---\\n\" + previous_attempts.to_string() + \"---\\n\")\n",
    "\n",
    "    if accuracy >= GOAL_ACCURACY:\n",
    "        break\n",
    "\n",
    "    # if i >= 0:\n",
    "    #     print(f\"TEMP: Stopping because we've tried {i+1} plans already.\")\n",
    "    #     break\n",
    "\n",
    "\n",
    "# print(f\"\\n\\nFinal prompt:\\n{prompt_generated_str}\")\n",
    "save_tmp_file(\"10-prompt_final.md\", prompt_str)\n",
    "print(f\"\\nFinal prompt saved with accuracy {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If PROMPT_TO_EVAL_FILE is not None, load the prompt from the file\n",
    "# prompt_str = \"\"\n",
    "# if PROMPT_TO_EVAL_FILE is not None:\n",
    "#     print(f\"Loading prompt from {PROMPT_TO_EVAL_FILE}\")\n",
    "#     with open(PROMPT_TO_EVAL_FILE, \"r\") as f:\n",
    "#         prompt_str = f.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
